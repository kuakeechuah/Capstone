{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Sentiment Analysis for CMON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 3 Jupyter notebooks for this project.\n",
    "\n",
    "\n",
    "## Notebook 1 Contents:\n",
    "- 1. Importing Libraries\n",
    "- 2. Scrapping and Saving Train/Test Dataset\n",
    "\n",
    "\n",
    "## Notebook 2 Contents:\n",
    "\n",
    "- Problem Statement\n",
    "- Executive Summary\n",
    "- Conclusions and Recommendations\n",
    "- 1. Importing Libraries\n",
    "- 2. Importing Data\n",
    "- 3. Data Cleansing\n",
    "    - Remove moderator posts\n",
    "    - Selecting the columns with useful text data\n",
    "    - Filling up null values\n",
    "    - Removing duplicated posts\n",
    "    - No outliers removed, but \"key words\"  would be treated in Data Preprocessing\n",
    "\n",
    "- 4. Data Preprocessing\n",
    "    - Remove html tags using beautifulsoup\n",
    "    - Lowercase all words and split up words\n",
    "    - Remove non-letters: Remove special characters and numbers\n",
    "    - Remove keywords that points to a specific subreddit\n",
    "    - Remove stopwords: These are common words that are not useful for text classification\n",
    "    - Lemmatize words: This will convert each word to its base form\n",
    "    - Rejoin words back into a string\n",
    "\n",
    "- 5. Exploratory Data Analysis\n",
    "    - Wordcloud\n",
    "    - Barcharts\n",
    "    - Distribution of Meaningful Words \n",
    "\n",
    "- 6. Saving and exporting of Train/test set\n",
    "- 7. Preparing and saving and exporting of holdout set in accordance to steps 1 to 4 above\n",
    "- 8. Topic modelling\n",
    "- 9. Success Evaluation\n",
    "- 10. Findings\n",
    "- 11. Conclusion and Recommendations\n",
    "- 12. Next Steps\n",
    "\n",
    "## Notebook 3 Contents:\n",
    "\n",
    "- 1. Importing Libraries\n",
    "- 2. Importing Data\n",
    "- 3. Modeling\n",
    "    - CountVectorizer & Logistic Regression\n",
    "    - TF-IDF & Logistic Regression   \n",
    "    - CountVectorizer & Naive Bayes \n",
    "    - TF-IDF & Naive Bayes  \n",
    "    - CountVectorizer & SVC\n",
    "    - TF-IDF & SVC\n",
    "    - LSTM\n",
    "- 4. Model Evaluation\n",
    "- 5. Selection of Production Model\n",
    "- 7. Identifying the Most Predictive Words\n",
    "- 8. Applying Chosen Model on Holdout Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Ankh: Gods of Egypt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '285967'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/285967?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/285967?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/285967?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/285967?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 4\n",
      "Total 222 comments scraped for <Ankh: Gods of Egypt>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Marvel United"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '298047'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/298047?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/298047?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/298047?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 3\n",
      "Total 109 comments scraped for <Marvel United>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Sheriff of Nottingham "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '298638'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/298638?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/298638?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 2\n",
      "Total 9 comments scraped for <Sheriff of Nottingham (2nd Edition)>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Trudvang Legends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '266064'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/266064?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/266064?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/266064?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 3\n",
      "Total 145 comments scraped for <Trudvang Legends>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for God of War: The Card Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '278120'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/278120?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/278120?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 2\n",
      "Total 76 comments scraped for <God of War: Das Kartenspiel>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Bloodborne: The Board Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '273330'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/273330?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/273330?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/273330?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/273330?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 4\n",
      "Total 201 comments scraped for <Bloodborne: The Board Game>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Foodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '280896'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/280896?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/280896?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 2\n",
      "Total 67 comments scraped for <Foodies>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Munchkin Dungeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '257001'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/257001?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/257001?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 2\n",
      "Total 63 comments scraped for <Munchkin Dungeon>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Project: ELITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '256999'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/256999?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/256999?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/256999?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 3\n",
      "Total 148 comments scraped for <Project: ELITE>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Starcadia Quest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '257193'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/257193?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/257193?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/257193?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 3\n",
      "Total 122 comments scraped for <Starcadia Quest>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Cthulhu-death-may-die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '253344'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/253344?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/253344?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/253344?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/253344?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/253344?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/253344?comments=1&page=6\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/253344?comments=1&page=7\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/253344?comments=1&page=8\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 8\n",
      "Total 619 comments scraped for <Cthulhu: Death May Die>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Wacky Races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '256729'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/256729?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/256729?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 2\n",
      "Total 88 comments scraped for <Los Autos Locos: El Juego de Mesa>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Blue Moon City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '21882'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=6\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=7\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=8\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=9\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=10\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=11\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=12\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=13\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=14\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=15\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=16\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=17\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=18\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=19\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=20\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=21\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=22\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=23\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/21882?comments=1&page=24\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 24\n",
      "Total 2250 comments scraped for <Blue Moon City>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Victorian Masterminds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '189453'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/189453?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/189453?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/189453?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 3\n",
      "Total 197 comments scraped for <Genios Victorianos>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Narcos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '253106'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/253106?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/253106?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/253106?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 3\n",
      "Total 104 comments scraped for <Narcos: Desková hra>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Sugar Blast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '261449'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/261449?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/261449?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 2\n",
      "Total 1 comments scraped for <Sugar Blast>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Kick-Ass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '250561'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/250561?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/250561?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 2\n",
      "Total 82 comments scraped for <Ha/Ver: A Társasjáték>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Gizmos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '246192'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/246192?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/246192?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/246192?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/246192?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/246192?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/246192?comments=1&page=6\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/246192?comments=1&page=7\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/246192?comments=1&page=8\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/246192?comments=1&page=9\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/246192?comments=1&page=10\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/246192?comments=1&page=11\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 11\n",
      "Total 915 comments scraped for <Gizmos>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '233868'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/233868?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/233868?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/233868?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/233868?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/233868?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 5\n",
      "Total 334 comments scraped for <HATE>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the-world-of-smog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '209324'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/209324?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/209324?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/209324?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/209324?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 4\n",
      "Total 250 comments scraped for <Moloch felemelkedése>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Rising-sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '205896'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=6\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=7\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=8\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=9\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=10\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=11\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=12\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=13\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=14\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=15\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=16\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=17\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=18\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=19\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=20\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=21\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=22\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=23\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=24\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=25\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=26\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/205896?comments=1&page=27\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 27\n",
      "Total 2542 comments scraped for <Felkelő Nap>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for zombicide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '113924'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=6\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=7\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=8\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=9\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=10\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=11\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=12\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=13\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=14\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=15\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=16\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=17\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=18\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=19\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=20\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=21\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=22\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=23\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=24\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=25\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=26\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/113924?comments=1&page=27\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 27\n",
      "Total 2533 comments scraped for <Zombicide>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for dream-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '232980'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/232980?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/232980?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 2\n",
      "Total 49 comments scraped for <Chyba śnisz!>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for modern-art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '40381'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/40381?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/40381?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/40381?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/40381?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/40381?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/40381?comments=1&page=6\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/40381?comments=1&page=7\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/40381?comments=1&page=8\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 8\n",
      "Total 635 comments scraped for <Duckomenta Art>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Massive Darkness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '197070'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/197070?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/197070?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/197070?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/197070?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/197070?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/197070?comments=1&page=6\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/197070?comments=1&page=7\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/197070?comments=1&page=8\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/197070?comments=1&page=9\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/197070?comments=1&page=10\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/197070?comments=1&page=11\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 11\n",
      "Total 985 comments scraped for <Massive Darkness>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Ethnos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '206718'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=6\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=7\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=8\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=9\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=10\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=11\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=12\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=13\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=14\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=15\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=16\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/206718?comments=1&page=17\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 17\n",
      "Total 1561 comments scraped for <Ethnos>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for The Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '172047'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/172047?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/172047?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/172047?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/172047?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/172047?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/172047?comments=1&page=6\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/172047?comments=1&page=7\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/172047?comments=1&page=8\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/172047?comments=1&page=9\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 9\n",
      "Total 751 comments scraped for <The Others>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Blood Rage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '170216'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=6\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=7\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=8\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=9\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=10\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=11\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=12\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=13\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=14\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=15\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=16\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=17\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=18\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=19\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=20\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=21\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=22\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=23\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=24\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=25\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=26\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=27\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=28\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=29\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=30\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=31\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=32\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=33\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=34\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=35\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=36\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=37\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=38\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=39\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=40\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=41\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=42\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=43\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=44\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=45\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=46\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=47\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=48\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/170216?comments=1&page=49\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 49\n",
      "Total 4748 comments scraped for <Blood Rage>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Arcadia Quest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '155068'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=6\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=7\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=8\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=9\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=10\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=11\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=12\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=13\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=14\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=15\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=16\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=17\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/155068?comments=1&page=18\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 18\n",
      "Total 1627 comments scraped for <Arcadia Quest>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for The Grizzled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '171668'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=5\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=6\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=7\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=8\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=9\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=10\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=11\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=12\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=13\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=14\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=15\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=16\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=17\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=18\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=19\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=20\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=21\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=22\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/171668?comments=1&page=23\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 23\n",
      "Total 2160 comments scraped for <The Grizzled>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for  A Song of Ice and Fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '223376'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/223376?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/223376?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/223376?comments=1&page=3\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/223376?comments=1&page=4\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 4\n",
      "Total 214 comments scraped for <Canción de hielo y fuego: El juego de miniaturas>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for  Wrath of kings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = '146451'\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/146451?comments=1&page=1\n",
      "Request successful. Start scrapping comments...\n",
      "Request response from https://www.boardgamegeek.com/xmlapi/boardgame/146451?comments=1&page=2\n",
      "Request successful. Start scrapping comments...\n",
      "No comments found in page 2\n",
      "Total 39 comments scraped for <Wrath of Kings>. End of Scrapping\n"
     ]
    }
   ],
   "source": [
    "comment_list = []\n",
    "name = ''\n",
    "while page_number != 0:\n",
    "    url = f'https://www.boardgamegeek.com/xmlapi/boardgame/{game_id}?comments=1&page={page_number}'\n",
    "    print(f'Request response from {url}')\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        print('Request successful. Start scrapping comments...')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        name = soup.find('name').text\n",
    "        comment = soup.find_all('comment')\n",
    "        if len(comment) != 0: # there are some comments available in the url\n",
    "            page_number += 1\n",
    "            for i in range(len(comment)):\n",
    "                comment_dict = {}\n",
    "                comment_dict['username'] = comment[i].attrs['username']\n",
    "                comment_dict['rating'] = comment[i].attrs['rating']\n",
    "                comment_dict['comment'] = comment[i].text\n",
    "                comment_list.append(comment_dict)\n",
    "        else:\n",
    "            print(f'No comments found in page {page_number}')\n",
    "            print(f'Total {len(comment_list)} comments scraped for <{name}>. End of Scrapping')\n",
    "            page_number = 0\n",
    "    else:\n",
    "        print('Getting response failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ' '.join(re.findall(r\"(?i)\\b[a-z]+\\b\", name)) #remove non alphabets\n",
    "comment_df.to_csv(f'{name} comments.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
